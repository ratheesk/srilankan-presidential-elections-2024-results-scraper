{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://results.elections.gov.lk/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the webpage!\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Print the title of the webpage to confirm parsing\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting All Island Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting All Island Final Results\n",
    "all_island_results_section = soup.find('h4', text='All Island final Result').find_next('table')\n",
    "\n",
    "all_island_results = []\n",
    "for row in all_island_results_section.find_all('tr')[1:]:  # Skip the header row\n",
    "    columns = row.find_all('td')\n",
    "    if len(columns) > 0:\n",
    "        candidate = columns[0].text.strip()\n",
    "        party = columns[1].text.strip()\n",
    "        total_votes = columns[2].text.strip().replace(',', '')\n",
    "        preferences = columns[3].text.strip().replace(',', '')\n",
    "        total = columns[4].text.strip().replace(',', '')\n",
    "\n",
    "        all_island_results.append({\n",
    "            'candidate': candidate,\n",
    "            'party': party,\n",
    "            'total_votes': int(total_votes),\n",
    "            'preferences': int(preferences),\n",
    "            'total': int(total)\n",
    "        })\n",
    "\n",
    "# Display the extracted All Island results\n",
    "for result in all_island_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting All Island Preference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting All Island Preference Results\n",
    "preference_results_section = soup.find('h4', text='All Island Preference Result').find_next('table')\n",
    "\n",
    "preference_results = []\n",
    "for row in preference_results_section.find_all('tr')[1:]:  # Skip the header row\n",
    "    columns = row.find_all('td')\n",
    "    if len(columns) > 0:\n",
    "        candidate = columns[0].text.strip()\n",
    "        party = columns[1].text.strip()\n",
    "        preferences = columns[2].text.strip().replace(',', '')\n",
    "\n",
    "        preference_results.append({\n",
    "            'candidate': candidate,\n",
    "            'party': party,\n",
    "            'preferences': int(preferences)\n",
    "        })\n",
    "\n",
    "# Display the extracted Preference results\n",
    "for result in preference_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting All Island Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting All Island Results\n",
    "all_island_results_section = soup.find('h4', text='All Island Results')\n",
    "\n",
    "# Check if the section is found\n",
    "if all_island_results_section is not None:\n",
    "    all_island_results_section = all_island_results_section.find_next('table')\n",
    "\n",
    "    all_island_results = []\n",
    "    for row in all_island_results_section.find_all('tr')[1:]:  # Skip the header row\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) > 0:\n",
    "            candidate = columns[0].text.strip()\n",
    "            party = columns[1].text.strip()\n",
    "            votes_received = columns[2].text.strip().replace(',', '')\n",
    "            percentage = columns[3].text.strip()\n",
    "\n",
    "            all_island_results.append({\n",
    "                'candidate': candidate,\n",
    "                'party': party,\n",
    "                'votes_received': int(votes_received),\n",
    "                'percentage': percentage\n",
    "            })\n",
    "\n",
    "    # Display the extracted All Island results\n",
    "    print(\"\\nExtracted All Island Results:\")\n",
    "    for result in all_island_results[:5]:  # Print first 5 results\n",
    "        print(f\"Candidate: {result['candidate']}, Party: {result['party']}, Votes Received: {result['votes_received']}, Percentage: {result['percentage']}\")\n",
    "else:\n",
    "    print(\"Section 'All Island Results' not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Combine the results into a single DataFrame\n",
    "final_results_df = pd.DataFrame(all_island_results)\n",
    "preference_results_df = pd.DataFrame(preference_results)\n",
    "\n",
    "# Save All Island Results to CSV\n",
    "final_results_df.to_csv('all_island_results.csv', index=False)\n",
    "\n",
    "# Save Preference Results to CSV\n",
    "preference_results_df.to_csv('preference_results.csv', index=False)\n",
    "\n",
    "# Optionally, print confirmation messages\n",
    "print(\"All Island Results saved to 'all_island_results.csv'\")\n",
    "print(\"Preference Results saved to 'preference_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Voting Details\n",
    "voting_details_section = soup.find(text='Valid Votes').find_parent('table')\n",
    "\n",
    "# Initialize a dictionary to hold voting details\n",
    "voting_details = {}\n",
    "\n",
    "# Extract details from the voting details section\n",
    "if voting_details_section is not None:\n",
    "    rows = voting_details_section.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) > 0:\n",
    "            label = columns[0].text.strip()\n",
    "            value = columns[1].text.strip().replace(',', '')\n",
    "            voting_details[label] = int(value)\n",
    "\n",
    "# Display the extracted voting details\n",
    "for key, value in voting_details.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert voting details to a DataFrame\n",
    "voting_details_df = pd.DataFrame(list(voting_details.items()), columns=['Detail', 'Value'])\n",
    "\n",
    "# Save Voting Details to CSV\n",
    "voting_details_df.to_csv('voting_details.csv', index=False)\n",
    "\n",
    "# Confirmation message\n",
    "print(\"Voting details saved to 'voting_details.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## District Preference Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# List of districts to scrape preference data\n",
    "districts = [\n",
    "    \"Colombo\", \"Gampaha\", \"Kalutara\", \"Mahanuwara\", \"Matale\", \n",
    "    \"Nuwaraeliya\", \"Galle\", \"Matara\", \"Hambantota\", \"Jaffna\", \n",
    "    \"Vanni\", \"Batticaloa\", \"Digamadulla\", \"Trincomalee\", \"Kurunegala\", \n",
    "    \"Puttalam\", \"Anuradhapura\", \"Polonnaruwa\", \"Badulla\", \n",
    "    \"Monaragala\", \"Ratnapura\", \"Kegalle\"\n",
    "]\n",
    "\n",
    "# Initialize a list to hold preference results for all districts\n",
    "all_district_results = []\n",
    "\n",
    "# Loop through each district to extract preference data\n",
    "for district in districts:\n",
    "    # Construct the URL for each district\n",
    "    url = f\"https://results.elections.gov.lk/district_preference.php?district={district}\"\n",
    "    \n",
    "    # Make a request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extracting preference results for each district\n",
    "        preference_results_section = soup.find('h4', text=f'District Preference Result of {district}').find_next('table')\n",
    "\n",
    "        # Extract the data from the table\n",
    "        if preference_results_section is not None:\n",
    "            for row in preference_results_section.find_all('tr')[1:]:  # Skip the header row\n",
    "                columns = row.find_all('td')\n",
    "                if len(columns) > 0:\n",
    "                    candidate = columns[0].text.strip()\n",
    "                    party = columns[1].text.strip()\n",
    "                    preferences = columns[2].text.strip().replace(',', '')\n",
    "\n",
    "                    all_district_results.append({\n",
    "                        'district': district,\n",
    "                        'candidate': candidate,\n",
    "                        'party': party,\n",
    "                        'preferences': int(preferences)\n",
    "                    })\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {district}\")\n",
    "\n",
    "# Convert the results to a DataFrame and save to CSV\n",
    "district_results_df = pd.DataFrame(all_district_results)\n",
    "district_results_df.to_csv('district_preference_results.csv', index=False)\n",
    "\n",
    "# Confirmation message\n",
    "print(\"District preference results saved to 'district_preference_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of districts to scrape postal votes data\n",
    "districts = [\n",
    "    \"Colombo\", \"Gampaha\", \"Kalutara\", \"Mahanuwara\", \"Matale\", \n",
    "    \"Nuwaraeliya\", \"Galle\", \"Matara\", \"Hambantota\", \"Jaffna\", \n",
    "    \"Vanni\", \"Batticaloa\", \"Digamadulla\", \"Trincomalee\", \"Kurunegala\", \n",
    "    \"Puttalam\", \"Anuradhapura\", \"Polonnaruwa\", \"Badulla\", \n",
    "    \"Monaragala\", \"Ratnapura\", \"Kegalle\"\n",
    "]\n",
    "\n",
    "# Initialize a list to hold postal votes results for all districts\n",
    "all_postal_votes_results = []\n",
    "\n",
    "# Loop through each district to extract postal votes data\n",
    "for district in districts:\n",
    "    # Construct the URL for postal votes in each district\n",
    "    url = f\"https://results.elections.gov.lk/division_results.php?district={district}&pd_division=Postal%20Votes\"\n",
    "    \n",
    "    # Make a request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract postal votes for each district\n",
    "        postal_votes_section = soup.find('h4', text=f'Election Result for {district} District: Postal Votes').find_next('table')\n",
    "\n",
    "        # Extract the data from the table\n",
    "        if postal_votes_section is not None:\n",
    "            for row in postal_votes_section.find_all('tr')[1:]:  # Skip the header row\n",
    "                columns = row.find_all('td')\n",
    "                if len(columns) > 0:\n",
    "                    candidate = columns[0].text.strip()\n",
    "                    party = columns[1].text.strip()\n",
    "                    votes_received = columns[2].text.strip().replace(',', '')\n",
    "\n",
    "                    all_postal_votes_results.append({\n",
    "                        'district': district,\n",
    "                        'candidate': candidate,\n",
    "                        'party': party,\n",
    "                        'votes_received': int(votes_received)\n",
    "                    })\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {district}\")\n",
    "\n",
    "# Convert the results to a DataFrame and save to CSV\n",
    "postal_votes_df = pd.DataFrame(all_postal_votes_results)\n",
    "postal_votes_df.to_csv('postal_votes_results.csv', index=False)\n",
    "\n",
    "# Confirmation message\n",
    "print(\"Postal votes results saved to 'postal_votes_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to hold polling divisions for all districts\n",
    "all_polling_divisions = []\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://results.elections.gov.lk\"\n",
    "\n",
    "# Loop through each district to extract polling divisions data\n",
    "for district in districts:\n",
    "    # Construct the URL for each district\n",
    "    url = f\"https://results.elections.gov.lk/district_results.php?district={district}\"\n",
    "    \n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract polling divisions from the section with the h4 heading \"Polling Divisions\"\n",
    "        polling_divisions_section = soup.find('h4', text='Polling Divisions')\n",
    "        \n",
    "        if polling_divisions_section is not None:\n",
    "            polling_divisions_links = polling_divisions_section.find_next('div', class_='mt-3').find_all('a', class_='result-link')\n",
    "            \n",
    "            # Extract the data from the links\n",
    "            for link in polling_divisions_links:\n",
    "                division_name = link.find('p', class_='fw-bold').text.strip()\n",
    "                \n",
    "                all_polling_divisions.append({\n",
    "                    'district': district,\n",
    "                    'polling_division': division_name,\n",
    "                })\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {district}\")\n",
    "\n",
    "# Convert the results to a DataFrame and save to CSV\n",
    "polling_divisions_df = pd.DataFrame(all_polling_divisions)\n",
    "polling_divisions_df.to_csv('polling_divisions.csv', index=False)\n",
    "\n",
    "# Confirmation message\n",
    "print(\"Polling divisions results saved to 'polling_divisions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Read the CSV file containing district and polling division data\n",
    "polling_divisions_df = pd.read_csv('polling_divisions.csv')\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://results.elections.gov.lk/division_results.php\"\n",
    "\n",
    "# Initialize a list to hold the election result data\n",
    "election_results = []\n",
    "\n",
    "# Function to scrape data from each polling division\n",
    "def scrape_polling_division_data(district, polling_division):\n",
    "    # Construct the URL for the polling division\n",
    "    url = f\"{base_url}?district={district}&pd_division={polling_division.replace(' ', '%20')}\"\n",
    "\n",
    "    # Send a request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the h4 title for the polling division\n",
    "        # Set title format based on whether it's postal votes or not\n",
    "        if polling_division == \"Postal Votes\":\n",
    "            h4_title = soup.find('h4', text=f'Election Result for {district} District: {polling_division}')\n",
    "        else:\n",
    "            h4_title = soup.find('h4', text=f'Election Result for Polling Division : {polling_division}')\n",
    "\n",
    "        if h4_title:\n",
    "            # Find the table containing the results (assuming it's the first table after the h4)\n",
    "            results_table = h4_title.find_next('table')\n",
    "\n",
    "            if results_table:\n",
    "                # Extract rows from the table\n",
    "                for row in results_table.find_all('tr')[1:]:  # Skip the header row\n",
    "                    columns = row.find_all('td')\n",
    "                    if len(columns) > 0:\n",
    "                        candidate = columns[0].text.strip()\n",
    "                        party = columns[1].text.strip()\n",
    "                        votes = columns[2].text.strip().replace(',', '')\n",
    "                        percentage = columns[3].text.strip()\n",
    "\n",
    "                        # Append the data to the election_results list\n",
    "                        election_results.append({\n",
    "                            'district': district,\n",
    "                            'polling_division': polling_division,\n",
    "                            'candidate': candidate,\n",
    "                            'party': party,\n",
    "                            'votes': int(votes),\n",
    "                            'percentage': percentage\n",
    "                        })\n",
    "        else:\n",
    "            print(f\"Title not found for {polling_division}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {polling_division} in {district}\")\n",
    "\n",
    "# Loop through each polling division and scrape the data\n",
    "for index, row in polling_divisions_df.iterrows():\n",
    "    district = row['district']\n",
    "    polling_division = row['polling_division']\n",
    "    print(f\"Scraping data for {polling_division} in {district}...\")\n",
    "    scrape_polling_division_data(district, polling_division)\n",
    "\n",
    "# Save the election results to a CSV file\n",
    "with open('polling_division_results.csv', mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['district', 'polling_division', 'candidate', 'party', 'votes', 'percentage'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(election_results)\n",
    "\n",
    "print(\"Election results saved to 'polling_division_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the CSV file containing district and polling division data\n",
    "polling_divisions_df = pd.read_csv('polling_divisions.csv')\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://results.elections.gov.lk/division_results.php\"\n",
    "\n",
    "# Initialize a list to hold the additional details\n",
    "additional_details = []\n",
    "\n",
    "# Function to scrape voting details from each polling division\n",
    "def scrape_voting_details(district, polling_division):\n",
    "    # Construct the URL for the polling division\n",
    "    url = f\"{base_url}?district={district}&pd_division={polling_division.replace(' ', '%20')}\"\n",
    "\n",
    "    # Send a request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract voting details (valid votes, rejected votes, etc.)\n",
    "        voting_details_section = soup.find(text='Valid Votes').find_parent('table')\n",
    "        \n",
    "        # Initialize a dictionary to hold voting details\n",
    "        voting_details = {}\n",
    "\n",
    "        # Extract details from the voting details section\n",
    "        if voting_details_section is not None:\n",
    "            rows = voting_details_section.find_all('tr')\n",
    "\n",
    "            for row in rows:\n",
    "                columns = row.find_all('td')\n",
    "                if len(columns) > 0:\n",
    "                    label = columns[0].text.strip()\n",
    "                    value = columns[1].text.strip().replace(',', '')\n",
    "                    voting_details[label] = int(value)\n",
    "\n",
    "            # Append additional data to the additional_details list\n",
    "            additional_details.append({\n",
    "                'district': district,\n",
    "                'polling_division': polling_division,\n",
    "                'valid_votes': voting_details.get('Valid Votes', 0),\n",
    "                'rejected_votes': voting_details.get('Rejected Votes', 0),\n",
    "                'total_polled': voting_details.get('Total Polled', 0),\n",
    "                'total_electors': voting_details.get('Total Electors', 0)\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Voting details table not found for {polling_division} in {district}.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {polling_division} in {district}. HTTP Status: {response.status_code}\")\n",
    "\n",
    "# Iterate through the polling divisions and scrape voting details\n",
    "for _, row in polling_divisions_df.iterrows():\n",
    "    district = row['district']\n",
    "    polling_division = row['polling_division']\n",
    "    print(f\"Scraping data for {polling_division} in {district}...\")\n",
    "\n",
    "    scrape_voting_details(district, polling_division)\n",
    "\n",
    "# Convert the list of additional details to a DataFrame and save to CSV\n",
    "additional_details_df = pd.DataFrame(additional_details)\n",
    "additional_details_df.to_csv('additional_polling_details.csv', index=False)\n",
    "\n",
    "print(\"Voting details scraping complete and saved to additional_polling_details.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the page and the folder to save the images\n",
    "url = 'https://results.elections.gov.lk/'\n",
    "symbols_folder = 'symbols'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(symbols_folder):\n",
    "    os.makedirs(symbols_folder)\n",
    "\n",
    "# Function to download an image\n",
    "def download_image(img_url, folder, img_name):\n",
    "    img_data = requests.get(img_url).content\n",
    "    with open(os.path.join(folder, img_name), 'wb') as handler:\n",
    "        handler.write(img_data)\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all img tags with the symbols\n",
    "    img_tags = soup.find_all('img', src=lambda src: src and 'symbols' in src)\n",
    "\n",
    "    # Loop through each img tag and download the images\n",
    "    for img_tag in img_tags:\n",
    "        img_url = img_tag['src']\n",
    "        img_name = img_url.split('/')[-1]  # Extract the file name\n",
    "\n",
    "        # Handle relative URLs\n",
    "        if not img_url.startswith('http'):\n",
    "            img_url = f\"https://results.elections.gov.lk/{img_url}\"  # Use the appropriate base URL\n",
    "\n",
    "        # Download and save the image\n",
    "        print(f\"Downloading {img_name} from {img_url}\")\n",
    "        download_image(img_url, symbols_folder, img_name)\n",
    "\n",
    "    print('All images downloaded successfully!')\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
